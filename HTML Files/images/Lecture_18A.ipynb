{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mX3Ph36PY-c2",
    "outputId": "9f47ac2b-8a87-4f8e-8518-d2ddd2d0a22d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.11\" 2021-04-20\n",
      "OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "# Summer 2021\n",
    "!java -version\n",
    "\n",
    "#Install Spark\n",
    "#download file\n",
    "!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
    "#extract the file\n",
    "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
    "#install findspark package\n",
    "!pip install -q findspark\n",
    "\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CU7Dwmo1ZFm1"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ueW5EvRNZKKI"
   },
   "outputs": [],
   "source": [
    "# create entry points to spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName(\"lecture18\").setMaster(\"local[*]\")#local[2]\n",
    "sc=SparkContext(conf = conf)\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouzX8I_ZGfx4"
   },
   "source": [
    "# Chapter 6. Working with Different Types of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szebwOeANoIZ",
    "outputId": "a83502b3-907b-4639-b4f7-18529b9c24dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDf = spark.range(10)\\\n",
    "    .withColumn(\"today\", current_date())\\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "dateDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2021-07-17|        2021-07-27|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, col\n",
    "dateDf.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|datediff(date_sub(today, 7), today)|\n",
      "+-----------------------------------+\n",
      "|                                 -7|\n",
      "+-----------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date, lit\n",
    "dateDf.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "dateDf.select(datediff(date_sub(col(\"today\"), 7), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2021-07-22|2021-07-22 08:42:...|\n",
      "|  1|2021-07-22|2021-07-22 08:42:...|\n",
      "|  2|2021-07-22|2021-07-22 08:42:...|\n",
      "|  3|2021-07-22|2021-07-22 08:42:...|\n",
      "|  4|2021-07-22|2021-07-22 08:42:...|\n",
      "|  5|2021-07-22|2021-07-22 08:42:...|\n",
      "|  6|2021-07-22|2021-07-22 08:42:...|\n",
      "|  7|2021-07-22|2021-07-22 08:42:...|\n",
      "|  8|2021-07-22|2021-07-22 08:42:...|\n",
      "|  9|2021-07-22|2021-07-22 08:42:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf.createOrReplaceTempView(\"dateTable\")\n",
    "spark.sql('select * from dateTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2021-07-22|2021-07-22 08:42:...|\n",
      "|  1|2021-07-22|2021-07-22 08:42:...|\n",
      "|  2|2021-07-22|2021-07-22 08:42:...|\n",
      "|  3|2021-07-22|2021-07-22 08:42:...|\n",
      "|  4|2021-07-22|2021-07-22 08:42:...|\n",
      "|  5|2021-07-22|2021-07-22 08:42:...|\n",
      "|  6|2021-07-22|2021-07-22 08:42:...|\n",
      "|  7|2021-07-22|2021-07-22 08:42:...|\n",
      "|  8|2021-07-22|2021-07-22 08:42:...|\n",
      "|  9|2021-07-22|2021-07-22 08:42:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf = dateDf.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n",
    "spark.sql('select * from dateTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).select(to_date(col(\"date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             Nikita|\n",
      "+-------------------+\n",
      "|2017-01-01 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT cast(to_date('2017-01-01', 'yyyy-mm-dd') as timestamp) as Nikita\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qza700tvNz1O",
    "outputId": "5a79e0d2-f203-4186-8a63-1fb91c19ec72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX2Z5KZKKPpI"
   },
   "source": [
    "# Working with Strings\n",
    "String manipulation shows up in nearly every data flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h21CXVqvJ9FB",
    "outputId": "bde59877-6d06-4a3c-e46a-94a66bf32ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "|Set 7 Babushka Nesting Boxes       |\n",
      "|Glass Star Frosted T-light Holder  |\n",
      "|Hand Warmer Union Jack             |\n",
      "|Hand Warmer Red Polka Dot          |\n",
      "|Assorted Colour Bird Ornament      |\n",
      "|Poppy's Playhouse Bedroom          |\n",
      "|Poppy's Playhouse Kitchen          |\n",
      "|Feltcraft Princess Charlotte Doll  |\n",
      "|Ivory Knitted Mug Cosy             |\n",
      "|Box Of 6 Assorted Colour Teaspoons |\n",
      "|Box Of Vintage Jigsaw Blocks       |\n",
      "|Box Of Vintage Alphabet Blocks     |\n",
      "|Home Building Block Word           |\n",
      "|Love Building Block Word           |\n",
      "|Recipe Box With Metal Heart        |\n",
      "+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The initcap function will capitalize every word in a given string when that word is separated from another by a space\n",
    "from pyspark.sql.functions import initcap, col\n",
    "df.select(initcap(col(\"Description\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29gwEkyL5kbF",
    "outputId": "cda859d6-4d8f-4f25-aa0c-aaf97f27413f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|initcap(Description)               |\n",
      "+-----------------------------------+\n",
      "|White Hanging Heart T-light Holder |\n",
      "|White Metal Lantern                |\n",
      "|Cream Cupid Hearts Coat Hanger     |\n",
      "|Knitted Union Flag Hot Water Bottle|\n",
      "|Red Woolly Hottie White Heart.     |\n",
      "|Set 7 Babushka Nesting Boxes       |\n",
      "|Glass Star Frosted T-light Holder  |\n",
      "|Hand Warmer Union Jack             |\n",
      "|Hand Warmer Red Polka Dot          |\n",
      "|Assorted Colour Bird Ornament      |\n",
      "|Poppy's Playhouse Bedroom          |\n",
      "|Poppy's Playhouse Kitchen          |\n",
      "|Feltcraft Princess Charlotte Doll  |\n",
      "|Ivory Knitted Mug Cosy             |\n",
      "|Box Of 6 Assorted Colour Teaspoons |\n",
      "|Box Of Vintage Jigsaw Blocks       |\n",
      "|Box Of Vintage Alphabet Blocks     |\n",
      "|Home Building Block Word           |\n",
      "|Love Building Block Word           |\n",
      "|Recipe Box With Metal Heart        |\n",
      "+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT initcap(Description) FROM dfTable\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4Q8n_i76C79",
    "outputId": "aa825527-e517-48a7-db81-8990bd0021a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |lower(Description)                |upper(lower(Description))         |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can cast strings in uppercase and lowercase\n",
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col(\"Description\"),\n",
    "    lower(col(\"Description\")),\n",
    "    upper(lower(col(\"Description\")))).show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlyYiH9G6azt",
    "outputId": "d37f7aa7-30af-4099-99f7-71c7abae7a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |lower(Description)                |upper(lower(Description))         |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Description, lower(Description), \\\n",
    "Upper(lower(Description)) FROM dfTable\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_Mcbewg6tm3"
   },
   "source": [
    "Another trivial task is adding or removing spaces around a string. You can do this by using lpad, ltrim, rpad and rtrim, trim\n",
    "* pyspark.sql.functions.lpad(col, len, pad)\n",
    "* Left-pad the string column to width len with pad.\n",
    "* In order to add padding to the left side of the column we use left pad of column in pyspark, left padding is accomplished using lpad() function. In order to add padding to the right side of the column we use right pad of column in pyspark, right padding is accomplished using rpad() function. Let’s see how to\n",
    "*  https://www.datasciencemadesimple.com/left-and-right-pad-of-column-in-pyspark-lpad-rpad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jRPGoPPr6lob",
    "outputId": "8693ef45-f3a4-4704-dd6a-88601c63bd67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+----------+----------+\n",
      "|    ltrim|    rtrim| trim|        lp|        rp|\n",
      "+---------+---------+-----+----------+----------+\n",
      "|HELLO    |    HELLO|HELLO|#####HELLO|HELLO#####|\n",
      "+---------+---------+-----+----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 10, \"#\").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \"#\").alias(\"rp\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ghgZi5c60Az",
    "outputId": "506e829b-6045-43b6-d2cf-bfd5dd6821c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------+---------------------+----------------------+-----------------------+\n",
      "|ltrim(    HELLLOOOO  )|rtrim(    HELLLOOOO  )|trim(    HELLLOOOO  )|lpad(HELLOOOO  , 3,  )|rpad(HELLOOOO  , 10,  )|\n",
      "+----------------------+----------------------+---------------------+----------------------+-----------------------+\n",
      "|           HELLLOOOO  |             HELLLOOOO|            HELLLOOOO|                   HEL|             HELLOOOO  |\n",
      "+----------------------+----------------------+---------------------+----------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT \\\n",
    "  ltrim('    HELLLOOOO  '), \\\n",
    "  rtrim('    HELLLOOOO  '), \\\n",
    "  trim('    HELLLOOOO  '), \\\n",
    "  lpad('HELLOOOO  ', 3, ' '), \\\n",
    "  rpad('HELLOOOO  ', 10, ' ') \\\n",
    "FROM dfTable\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmvyuPwX79jo"
   },
   "source": [
    "Note that if lpad or rpad takes a number less than the length of the string, it will always remove values from the right side of the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sNsnPID8Azf"
   },
   "source": [
    "## Regular Expressions\n",
    "* Probably one of the most frequently performed tasks is searching for the existence of one string in another or replacing all mentions of a string with another value. This is often done with a tool called regular expressions that exists in many programming languages.\n",
    "* There are two key functions in Spark that you’ll need in order to perform regular expression tasks: regexp_extract and regexp_replace. These functions extract values and replace values, respectively.\n",
    "* Regular expressions in Java \n",
    "* https://www.vogella.com/tutorials/JavaRegularExpressions/article.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKuLPc407z4j",
    "outputId": "e5277b93-a3b9-46d5-fa00-9b73311c06a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(\n",
    "  regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "  col(\"Description\")).show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "misobWzD-SZl",
    "outputId": "e9eed09a-e8ea-4e06-bb17-42dedab983df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|color_clean                       |Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|COLOR METAL LANTERN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as \\\n",
    "  color_clean, Description FROM dfTable\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1d8h0eRCBOH"
   },
   "source": [
    "Another task might be to replace given characters with other characters. Building this as a regular expression could be tedious, so Spark also provides the translate function to replace these values. This is done at the character level and will replace all instances of a character with the indexed character in the replacement string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L7oFFVVBqnU",
    "outputId": "79294ad2-ff99-440a-bea2-d8135927d622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|translate(Description, LEET, 1337)|Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n",
    "  .show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpFAkNNeCEE2",
    "outputId": "bda6852e-132f-4382-c335-bfd8e95f90a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|translate(Description, LEET, 1337)|Description                       |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT translate(Description, 'LEET', '1337'), Description FROM dfTable\").show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PMY29dtCe_l"
   },
   "source": [
    "* We can also perform something similar, like pulling out the first mentioned color\n",
    "* pyspark.sql.functions.regexp_extract(str, pattern, idx)\n",
    "* Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxrDwlAd44RO"
   },
   "source": [
    "Parameter\tType\tDescription\n",
    "* **input**\tstring the string to search for strings matching the regular expression.\n",
    "* **regex** string the regular expression to match\n",
    "* **group**\tinteger an optional regular expression group number, defining which portion of the matching string will be returned\n",
    "* https://docs.data.world/documentation/sql/reference/functions/regexp_extract.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBwc8OmwCJD6",
    "outputId": "d2414c9b-bc98-40d6-aac3-bde7d73f3237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------------+\n",
      "|color_clean|Description                       |\n",
      "+-----------+----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE      |WHITE METAL LANTERN               |\n",
      "+-----------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "     regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),\n",
    "     col(\"Description\")).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSTepC6IChue",
    "outputId": "3eaedcc6-6f61-4b0d-f3b9-f9b6c56ae73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+--------------------+\n",
      "|regexp_extract(Description, (BLACK|WHITE|RED|GREEN|BLUE), 1)|         Description|\n",
      "+------------------------------------------------------------+--------------------+\n",
      "|                                                       WHITE|WHITE HANGING HEA...|\n",
      "|                                                       WHITE| WHITE METAL LANTERN|\n",
      "+------------------------------------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT regexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1), \\\n",
    "  Description FROM dfTable\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhubrNHc1rTg",
    "outputId": "83e354ea-220a-4b97-a3c3-fcf74d0822e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|          str|\n",
      "+-------------+\n",
      "|salem 100-200|\n",
      "+-------------+\n",
      "\n",
      "+---+\n",
      "|  d|\n",
      "+---+\n",
      "|100|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([('salem 100-200',)], ['str'])\n",
    "df2.show()\n",
    "df2.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).show() #('(\\d+)-(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgnyyPe44adR",
    "outputId": "7eb8fd9a-1e6b-4533-96e5-e5dfc85f7f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  d|\n",
      "+---+\n",
      "|200|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(regexp_extract('str', r'(\\d+)-(\\d+)', 2).alias('d')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCz4jZHh4idX",
    "outputId": "256c34c4-dc06-4fae-cf21-393f6c23622b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      d|\n",
      "+-------+\n",
      "|100-200|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(regexp_extract('str', r'((\\d+)-(\\d+))', 1).alias('d')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZuMKQzz2zsn"
   },
   "source": [
    "* ( start a capture group\n",
    "* \\d a shorthand character class, which matches all numbers; it is the same as [0-9]\n",
    "* http://www.regular-expressions.info/charclass.html\n",
    "* + one or more of the expression\n",
    "* ) end a capture group\n",
    "* / a literal forward slash\n",
    "* If you remove the brackets (), the expression will still match, but you'll only capture one set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8YfByNz2us8",
    "outputId": "5d451aef-f80d-4429-925b-8aad6759e26c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|str|\n",
      "+---+\n",
      "|foo|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "|  d|\n",
      "+---+\n",
      "|   |\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([('foo',)], ['str'])# foo10\n",
    "df2.show()\n",
    "df2.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBWmOFpW3S0m",
    "outputId": "096f9bf8-8733-4a9d-9746-877735b8ca9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   d|\n",
      "+----+\n",
      "|aaaa|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([('aaaac',)], ['str'])\n",
    "df2.select(regexp_extract('str', '(a+)(b)?(c)', 1).alias('d')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhT1wphY6Uj6",
    "outputId": "078de2ec-c7b7-4ed7-f704-d68c6e0cdc77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------+\n",
      "|ID  |Notes                                     |\n",
      "+----+------------------------------------------+\n",
      "|2345|Checked by John                           |\n",
      "|2398|Verified by Stacy                         |\n",
      "|2328|Verified by Srinivas than some random text|\n",
      "|3983|Double Checked on 2/23/17 by Marsha       |\n",
      "+----+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('2345', 'Checked by John'),\n",
    "('2398', 'Verified by Stacy'),\n",
    "('2328', 'Verified by Srinivas than some random text'),        \n",
    "('3983', 'Double Checked on 2/23/17 by Marsha')]\n",
    "\n",
    "df2 = sc.parallelize(data).toDF(['ID', 'Notes'])\n",
    "\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Up-FC_3a6eDZ",
    "outputId": "22964b72-0580-44ae-d267-dd9f8b45913e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------+--------+\n",
      "|ID  |Notes                                     |Employee|\n",
      "+----+------------------------------------------+--------+\n",
      "|2345|Checked by John                           |John    |\n",
      "|2398|Verified by Stacy                         |Stacy   |\n",
      "|2328|Verified by Srinivas than some random text|Srinivas|\n",
      "|3983|Double Checked on 2/23/17 by Marsha       |Marsha  |\n",
      "+----+------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df2.withColumn('Employee', regexp_extract(col('Notes'), '(.)(by)(\\s+)(\\w+)', 4))\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM4YAQ4E6jyf"
   },
   "source": [
    "Here regex('(.)(by)(\\s+)(\\w+)') means\n",
    "\n",
    "* (.) - Any character (except newline)\n",
    "* (by) - Word by in the text\n",
    "* (\\s+) - One or many spaces\n",
    "* (\\w+) - Alphanumeric or underscore chars of length one; [a-z,A-Z,_] (the equivalent of \\w) means a to z and A to Z - it isn't the literal hyphen in this context, if you did want a hyphen, put it as the first or last character of a character class.\n",
    "* and group_number is 4 because group (\\w+) is in 4th position in expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow7T65toEtXG"
   },
   "source": [
    "Sometimes, rather than extracting values, we simply want to check for their existence. We can do this with the contains method on each column. This will return a Boolean declaring whether the value you specify is in the column’s string. In Python and SQL, we can use the instr function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHOiZPKGDTEd",
    "outputId": "987e5695-73d1-44ca-dae4-ee89377fb1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite) \\\n",
    "  .where(\"hasSimpleColor\")\\\n",
    "  .select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnuU2dzWFbkp",
    "outputId": "e3a996d0-bfa5-4391-c0dd-71ac7eeb0cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Description FROM dfTable \\\n",
    "WHERE instr(Description, 'BLACK') >= 1 OR instr(Description, 'WHITE') >= 1\").show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFwQj6BcFp_g"
   },
   "source": [
    "This is trivial with just two values, but it becomes more complicated when there are values.\n",
    "* locate returns the integer location (1 based location)\n",
    "* pyspark.sql.functions.locate(substr, str, pos=1)\n",
    "* Locate the position of the first occurrence of substr in a string column, after position pos\n",
    "* You need to subtract 1 because string index starts from 1, not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9RAoElmLz3t",
    "outputId": "9bad6451-5381-49f5-b2cf-06c038b1e5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   s|\n",
      "+----+\n",
      "|abcd|\n",
      "| 123|\n",
      "+----+\n",
      "\n",
      "+---+\n",
      "|  s|\n",
      "+---+\n",
      "|  2|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import locate\n",
    "df7 = spark.createDataFrame([('abcd',),('123',)], ['s',])\n",
    "df7.show()\n",
    "df7.select(locate('b', df7.s, 1).alias('s')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMxYmUeCanfv",
    "outputId": "e68b7c53-abeb-4b79-9c8b-7366aac8adef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------+\n",
      "|tweet                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------+\n",
      "|@__therealanna @heyitsrose let's have a zoom meeting tonite! #quarantinelife #girlsnight #onlinehangout|\n",
      "+-------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet = \"@__therealanna @heyitsrose let's have a zoom meeting tonite! #quarantinelife #girlsnight #onlinehangout\"\n",
    "df7 = spark.createDataFrame([(tweet,)], ['tweet',])\n",
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmaaHqxRbS7E",
    "outputId": "909b78b6-07ed-45cb-c700-43e73b58df49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------+-------------+\n",
      "|tweet                                                                                                  |Mentions     |\n",
      "+-------------------------------------------------------------------------------------------------------+-------------+\n",
      "|@__therealanna @heyitsrose let's have a zoom meeting tonite! #quarantinelife #girlsnight #onlinehangout|__therealanna|\n",
      "+-------------------------------------------------------------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract,col\n",
    "\n",
    "result = df7.withColumn('Mentions', regexp_extract(col('tweet'), '@([a-zA-Z0-9_]{1,50})', 1))\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKJehUgqd9xz",
    "outputId": "67bc9767-6009-43f4-ae11-0d2846f09fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|col                  |\n",
      "+---------------------+\n",
      "|2 AVENUE DES LAPINOUS|\n",
      "+---------------------+\n",
      "\n",
      "+---------------------+--------------------------+\n",
      "|col                  |output                    |\n",
      "+---------------------+--------------------------+\n",
      "|2 AVENUE DES LAPINOUS|[2, AVENUE, DES, LAPINOUS]|\n",
      "+---------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  pyspark.sql.functions as F\n",
    " \n",
    "df = spark.createDataFrame([('2 AVENUE DES LAPINOUS',)], ['col'])\n",
    "df.show(truncate=False)\n",
    "df = df.withColumn('output', F.expr(\"regexp_extract_all(col, '(\\\\\\\\w+)', 1)\"))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEChHYtTfxxf",
    "outputId": "7f5f586b-2d88-4b62-c5e1-a2b792ac3f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "|tweet                                                                                                  |Mentions                   |\n",
      "+-------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "|@__therealanna @heyitsrose let's have a zoom meeting tonite! #quarantinelife #girlsnight #onlinehangout|[__therealanna, heyitsrose]|\n",
      "+-------------------------------------------------------------------------------------------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df7.withColumn('Mentions', F.expr(\"regexp_extract_all(tweet, '@([a-zA-Z0-9_]{1,50})', 1)\"))\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEhqTEkKFj6H",
    "outputId": "1d8ba750-e3ff-481a-c6ce-86c11a054722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|HAND WARMER RED POLKA DOT         |\n",
      "|RED COAT RACK PARIS FASHION       |\n",
      "+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "def color_locator(column, color_string):\n",
    "  #x=locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)\n",
    "  #df.select(x).show()\n",
    "  return locate(color_string.upper(), column)\\\n",
    "          .cast(\"boolean\")\\\n",
    "          .alias(\"is_\" + color_string)\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"Description\")) # has to a be Column type\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
    "  .select(\"Description\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lm0bSXoandy4",
    "outputId": "ff3217a5-b0e8-47c4-99f3-fef4ac021498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[1,2,3,4]\n",
    "x.append(7)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQxow1kNUz4N",
    "outputId": "141e9763-65e8-4ae5-85cf-3da3cde622fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'CAST(locate(BLACK, Description, 1) AS BOOLEAN) AS `is_black`'> Column<'CAST(locate(WHITE, Description, 1) AS BOOLEAN) AS `is_white`'> Column<'CAST(locate(RED, Description, 1) AS BOOLEAN) AS `is_red`'> Column<'CAST(locate(GREEN, Description, 1) AS BOOLEAN) AS `is_green`'> Column<'CAST(locate(BLUE, Description, 1) AS BOOLEAN) AS `is_blue`'> Column<'unresolvedstar()'>\n"
     ]
    }
   ],
   "source": [
    "print(*selectedColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lee_PbTYK-bJ",
    "outputId": "b012bafc-9fa8-4cab-851f-677cc7381ff0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(*selectedColumns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3js_5QmGmYz",
    "outputId": "dbb7f9ba-3c30-451a-c0c4-8f1262c258a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLw4_40-v6yB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture 18A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
